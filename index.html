<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="种一棵树最好的时间是现在">
<meta property="og:type" content="website">
<meta property="og:title" content="不就熬个夜">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="不就熬个夜">
<meta property="og:description" content="种一棵树最好的时间是现在">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="yi-yi1">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>不就熬个夜</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">不就熬个夜</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/23/Pytorch5-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/23/Pytorch5-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">Pytorch5-分类问题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-01-23 20:39:58" itemprop="dateCreated datePublished" datetime="2023-01-23T20:39:58+08:00">2023-01-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-24 16:14:10" itemprop="dateModified" datetime="2023-01-24T16:14:10+08:00">2023-01-24</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>​        分类问题中，实际上是对概率的计算与比较，而非类别之间的数值比较。</p>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><h2 id="二分类问题"><a href="#二分类问题" class="headerlink" title="二分类问题"></a>二分类问题</h2><p>​        二分类问题是非0即1的问题，由于隐藏条件的限制，有：</p>
<script type="math/tex; mode=display">
P(\widehat y=1)+P(\widehat y=0) = 1</script><p>对于二分类问题结果的预测，仅需要计算在0或1的条件下即可得到答案，通常计算1的概率。</p>
<h2 id="逻辑函数"><a href="#逻辑函数" class="headerlink" title="逻辑函数"></a>逻辑函数</h2><p>​        在原先的回归问题中，所用的模型为</p>
<script type="math/tex; mode=display">
\widehat y = \omega x+b</script><p>此时$\widehat y \in R$，但当问题为分类问题时，所求结果的值域应当发生变化，变为一个即$\widehat y \in [0,1]$，因此需要引入逻辑函数(sigmod)来实现。</p>
<script type="math/tex; mode=display">
\sigma(x) = \frac{1}{1+e^{-x}}</script><p>本函数原名为logistics函数，属于sigmoid类函数，由于其特性优异，代码中sigmoid函数就指的是本函数，图像为</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230123205841.png"/></p>
<p>特点：</p>
<ol>
<li>函数值在0到1之间变化明显，即导数大</li>
<li>在趋于0和1处函数逐渐平滑，即导数小</li>
<li>函数为饱和函数</li>
<li>单调函数</li>
</ol>
<p>除此之外，还有其他类的sigmoid函数。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230123211101.png"/></p>
<h2 id="模型变换"><a href="#模型变换" class="headerlink" title="模型变换"></a>模型变换</h2><p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230124160921.png"/></p>
<h3 id="Loss变化"><a href="#Loss变化" class="headerlink" title="Loss变化"></a>Loss变化</h3><p>​        原先是计算两个标量数值间的差距，也就是数轴上的距离。计算两个概率之间的差异，需要利用到交叉熵的理论。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230124161037.png"/></p>
<p>​        如下图所示，$\widehat y$与y越接近，BCE Loss越小</p>
<h2 id="整体代码"><a href="#整体代码" class="headerlink" title="整体代码"></a>整体代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>],[<span class="number">2.0</span>],[<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">0.0</span>],[<span class="number">0.0</span>],[<span class="number">1.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#改用LogisticRegressionModel 同样继承于Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LogisticRegressionModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LogisticRegressionModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment">#对原先的linear结果进行sigmod激活</span></span><br><span class="line">        y_pred = F.sigmoid(self.linear(x))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line">model = LogisticRegressionModel()</span><br><span class="line"></span><br><span class="line"><span class="comment">#构造的criterion对象所接受的参数为（y&#x27;,y） 改用BCE</span></span><br><span class="line">criterion = torch.nn.BCELoss(size_average=<span class="literal">False</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch,loss)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w = &#x27;</span>, model.linear.weight.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b = &#x27;</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y_pred = &#x27;</span>,y_test.data)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/23/Pytorch4-%E5%9F%BA%E4%BA%8EPytorch%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/23/Pytorch4-%E5%9F%BA%E4%BA%8EPytorch%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" class="post-title-link" itemprop="url">Pytorch4-基于Pytorch的线性模型</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-01-23 14:05:44 / 修改时间：20:42:19" itemprop="dateCreated datePublished" datetime="2023-01-23T14:05:44+08:00">2023-01-23</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="利用pytorch进行深度学习"><a href="#利用pytorch进行深度学习" class="headerlink" title="利用pytorch进行深度学习"></a>利用pytorch进行深度学习</h1><ul>
<li>准备数据集</li>
<li>设计用于计算最终结果的模型</li>
<li>构造损失函数及优化器</li>
<li>设计循环周期Training cycle——==前馈、反馈、更新==</li>
</ul>
<p>​        x,y∈R，在pytorch中，若使用mini-batch的算法，一次性求出一个批量的y，则需要x以及y作为矩阵参与计算，此时利用其广播机制，可以将原标量参数ω扩写为同维的矩阵[ω]，参与运算而不改变其Tensor的性质，对于矩阵，行表示样本，列表示特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#数据作为矩阵参与Tensor计算</span></span><br><span class="line">x_data = torch.Tensor([<span class="number">1.0</span>],[<span class="number">2.0</span>],[<span class="number">3.0</span>])</span><br><span class="line">y_data = torch.Tensor([<span class="number">2.0</span>],[<span class="number">4.0</span>],[<span class="number">6.0</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>​        ω以及b是需要反复训练确定的，在设计时，需要率先设计出此二者的维度。只要确定了y以及x的维度，就可以确定ω以及b的维度大小。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230123192843.png"></p>
<p>​        由于前边的计算过程都是针对矩阵的，因此最后的loss也是矩阵，但由于进行反向传播调整参数，因此loss应当是个标量，因此要对矩阵[loss]内每个量求和均值(MSE)</p>
<script type="math/tex; mode=display">
loss = \frac{1}{N}\Sigma
\begin{bmatrix}
{loss_1}\\
{\vdots}\\
{loss_n}\\
\end{bmatrix}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#固定继承于Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="comment">#构造函数初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment">#调用父类的init</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        <span class="comment">#Linear对象包括weight(w)以及bias(b)两个成员张量</span></span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#前馈函数forward，对父类函数中的overwrite</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment">#调用linear中的call()，以利用父类forward()计算wx+b</span></span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line">    <span class="comment">#反馈函数backward由module自动根据计算图生成</span></span><br><span class="line">model = LinearModel()</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230123195256.png"></p>
<h1 id="构造loss和optimizer"><a href="#构造loss和optimizer" class="headerlink" title="构造loss和optimizer"></a>构造loss和optimizer</h1><h2 id="loss计算"><a href="#loss计算" class="headerlink" title="loss计算"></a>loss计算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>​        其中MSELoss也是继承于nn.Modules的。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230123195512.png"></p>
<h2 id="optimizer计算"><a href="#optimizer计算" class="headerlink" title="optimizer计算"></a>optimizer计算</h2><p>​        优化器并不构建计算图，生产的优化器对象可以直接对整个模型进行优化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#model.parameters()用于检查模型中所能进行优化的张量</span></span><br><span class="line"><span class="comment">#learningrate(lr)表学习率，可以统一也可以不统一</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>
<h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><ol>
<li>前馈计算预测值与损失函数</li>
<li>梯度清零并进行backward</li>
<li>更新参数</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="comment">#前馈计算y_pred</span></span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    <span class="comment">#前馈计算损失loss</span></span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="comment">#打印调用loss时，会自动调用内部__str__()函数，避免产生计算图</span></span><br><span class="line">    <span class="built_in">print</span>(epoch,loss)</span><br><span class="line">    <span class="comment">#梯度清零</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment">#梯度反向传播，计算图清除</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment">#根据传播的梯度以及学习率更新参数</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<h2 id="测试模型"><a href="#测试模型" class="headerlink" title="测试模型"></a>测试模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">#Output</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'w = '</span>, model.linear.weight.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'b = '</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line"><span class="comment">#TestModel</span></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'y_pred = '</span>,y_test.data)</span><br></pre></td></tr></table></figure>
<h1 id="整体代码"><a href="#整体代码" class="headerlink" title="整体代码"></a>整体代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#数据作为矩阵参与Tensor计算</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>],[<span class="number">2.0</span>],[<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">2.0</span>],[<span class="number">4.0</span>],[<span class="number">6.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#固定继承于Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="comment">#构造函数初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment">#调用父类的init</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        <span class="comment">#Linear对象包括weight(w)以及bias(b)两个成员张量</span></span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#前馈函数forward，对父类函数中的overwrite</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment">#调用linear中的call()，以利用父类forward()计算wx+b</span></span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line">    <span class="comment">#反馈函数backward由module自动根据计算图生成</span></span><br><span class="line">model = LinearModel()</span><br><span class="line"></span><br><span class="line"><span class="comment">#构造的criterion对象所接受的参数为（y',y）</span></span><br><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#model.parameters()用于检查模型中所能进行优化的张量</span></span><br><span class="line"><span class="comment">#learningrate(lr)表学习率，可以统一也可以不统一</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment">#前馈计算y_pred</span></span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    <span class="comment">#前馈计算损失loss</span></span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="comment">#打印调用loss时，会自动调用内部__str__()函数，避免产生计算图</span></span><br><span class="line">    <span class="built_in">print</span>(epoch,loss)</span><br><span class="line">    <span class="comment">#梯度清零</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment">#梯度反向传播，计算图清除</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment">#根据传播的梯度以及学习率更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"> <span class="comment">#Output</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'w = '</span>, model.linear.weight.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'b = '</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line"><span class="comment">#TestModel</span></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'y_pred = '</span>,y_test.data)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/21/Pytorch3-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/21/Pytorch3-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/" class="post-title-link" itemprop="url">Pytorch3-反向传播</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-01-21 09:53:45 / 修改时间：11:50:07" itemprop="dateCreated datePublished" datetime="2023-01-21T09:53:45+08:00">2023-01-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>​        在线性模型中，</p>
<script type="math/tex; mode=display">
\widehat y = \omega x</script><p>如果</p>
<p>以神经网络的视角带入来看，则x为输入层，即input层，ω为<strong>权重</strong>。在神经网络中，通常将权重以及乘法计算操作的部分合并看做一个神经元（层）。而神经网络的训练过程即为更新权重的过程，其更新的情况依赖于</p>
<script type="math/tex; mode=display">
\frac{\partial loss}{\partial \omega}</script><p>,而并非</p>
<script type="math/tex; mode=display">
\frac{\partial \widehat y}{\partial \omega}</script><p>​        神经网络视角下的线性模型：</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230121104631.png"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">x~i~</td>
<td style="text-align:center">输入层的第i个结点</td>
</tr>
<tr>
<td style="text-align:center">h~ij~</td>
<td style="text-align:center">第i层隐含层的第j个结点</td>
</tr>
<tr>
<td style="text-align:center">o~i~</td>
<td style="text-align:center">输出层的第i个结点</td>
</tr>
<tr>
<td style="text-align:center">ω~x1~^mn^</td>
<td style="text-align:center">输入层的第m个结点与隐含层的第n个结点之间的权重</td>
</tr>
<tr>
<td style="text-align:center">ω~ij~^mn^</td>
<td style="text-align:center">隐含层第i层的第m个结点与第j层的第n个结点之间的权重</td>
</tr>
<tr>
<td style="text-align:center">ω~ko~^mn^</td>
<td style="text-align:center">隐含层最后一层（第k层）的第m个结点与输出层第n个结点之间的权重</td>
</tr>
</tbody>
</table>
</div>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230121110246.png"></p>
<p>​        上图中，输入层与隐藏层第一层之间有5×6=30个权重，而隐藏层第一层和隐藏层第二层之间由6×7=42个权重</p>
<h2 id="计算图中的神经网络"><a href="#计算图中的神经网络" class="headerlink" title="计算图中的神经网络"></a>计算图中的神经网络</h2><p>​        在计算图中，绿色的模块为计算模块，可以在计算过程中求导。MM为矩阵乘法，ADD为加法。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230121111341.png"></p>
<p>对于上图左式，可以化简得到如下公式：</p>
<script type="math/tex; mode=display">
\widehat y = W_2(W_1X+b_1)+b_2=W_2W_1X+(W_2b_1+b_2)=WX+b</script><p>也就是说，在这个结构下单纯的增加层数，并不能怎加神经网络的复杂程度，因为</p>
<p>最后都可以化简为一个单一的神经网络。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230121112009.png"></p>
<h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><p>​        在每层网结构中，增加一个非线性的==变换函数即激活函数==</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230121112314.png"></p>
<h1 id="反向传播过程"><a href="#反向传播过程" class="headerlink" title="反向传播过程"></a>反向传播过程</h1><h2 id="前馈计算"><a href="#前馈计算" class="headerlink" title="前馈计算"></a>前馈计算</h2><p>​        在某一处神经元，输入的x与ω经过函数f(x,ω)的计算，可以获得输出值z，并继续向前以得到损失值loss。在f(x,ω)的计算模块中会计算导数</p>
<script type="math/tex; mode=display">
\frac{\partial z}{\partial x}和\frac{\partial z}{\partial \omega}</script><p>并将其保存下来，在pytorch中，这样的值保存在变量x以及ω中。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230121113244.png"></p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>​        由于求导的链式法则，求得loss以后，前面的神经元会将</p>
<script type="math/tex; mode=display">
\frac{\partial loss}{\partial z}</script><p>的值反向传播给原先的神经元，在计算单元f(x,ω)中，将得到的</p>
<script type="math/tex; mode=display">
\frac{\partial loss}{\partial x}</script><p>与之前存储的导数相乘，即可得到损失值对于权重以及输入层的导数，即</p>
<script type="math/tex; mode=display">
\frac{\partial loss}{\partial x}和\frac{\partial loss}{\partial \omega}</script><p>基于该梯度才进行权重的调整。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230121114218.png"></p>
<h1 id="Pytorch中的前馈与反馈"><a href="#Pytorch中的前馈与反馈" class="headerlink" title="Pytorch中的前馈与反馈"></a>Pytorch中的前馈与反馈</h1><p>​        利用Pytorch框架进行深度学习，最主要的是==构建计算图==</p>
<h2 id="Tensor张量"><a href="#Tensor张量" class="headerlink" title="Tensor张量"></a>Tensor张量</h2><p>​        Tensor中重要的两个成员，data用于保存权重本身的值ω，grad用于保存损失函数对权重的导数，grad本身也是个张量。对张量进行的计算操作，都是建立计算图的过程。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230121114718.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#赋予tensor中的data</span></span><br><span class="line">w = torch.Tensor([<span class="number">1.0</span>])</span><br><span class="line"><span class="comment">#设定需要计算梯度grad</span></span><br><span class="line">w.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#模型y=x*w 建立计算图</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    w为Tensor类型</span></span><br><span class="line"><span class="string">    x强制转换为Tensor类型</span></span><br><span class="line"><span class="string">    通过这样的方式建立计算图</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (<span class="string">"predict  (before training)"</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        <span class="comment">#创建新的计算图</span></span><br><span class="line">        l = loss(x,y)</span><br><span class="line">        <span class="comment">#进行反馈计算，此时才开始求梯度，此后计算图进行释放</span></span><br><span class="line">        l.backward()</span><br><span class="line">        <span class="comment">#grad.item()取grad中的值变成标量</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'\tgrad:'</span>,x, y, w.grad.item())</span><br><span class="line">        <span class="comment">#单纯的数值计算要利用data，而不能用张量，否则会在内部创建新的计算图</span></span><br><span class="line">        w.data = w.data - <span class="number">0.01</span> * w.grad.data</span><br><span class="line">        <span class="comment">#把权重梯度里的数据清零</span></span><br><span class="line">        w.grad.data.zero_()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"progress:"</span>,epoch, l.item())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"predict (after training)"</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/20/Nilearn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/20/Nilearn/" class="post-title-link" itemprop="url">Nilearn</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-01-20 10:55:48" itemprop="dateCreated datePublished" datetime="2023-01-20T10:55:48+08:00">2023-01-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-21 10:48:29" itemprop="dateModified" datetime="2023-01-21T10:48:29+08:00">2023-01-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Nilearn-库介绍"><a href="#Nilearn-库介绍" class="headerlink" title="Nilearn 库介绍"></a>Nilearn 库介绍</h1><p>​        Nilearn 是一个将机器学习、模式识别、多变量分析等技术应用于神经影像数据的应用中，能完成多体素模式分析、解码、模型预测、构造功能连接、脑区分割、构造连接体等功能。一般用于处理功能磁共振图像（fMRI）、静息状态，或者基于体素的形态学分析。Nilearn的价值体现在特定领域特定工程的构造，将神经影像数据表达为非常合适于统计学习的特征矩阵。</p>
<h1 id="从fMRI数据到时间序列"><a href="#从fMRI数据到时间序列" class="headerlink" title="从fMRI数据到时间序列"></a>从fMRI数据到时间序列</h1><p>​        fMRI数据是4维的，包含三维的空间序列和一维的时间，在实际应用中，更多的是利用大脑图像时间序列做研究，无法直接使用fMRI数据做相关研究，需要对原始数据做一些数据预处理和变换。</p>
<h2 id="mask"><a href="#mask" class="headerlink" title="mask"></a>mask</h2><p>​        第一步所做的都是把四维的fMRI数据转换为二维矩阵，这个过程称为mask，就是提取能利用的特征，通过mask得到的二维矩阵包含一维的时间和一维的特征，将fMRI数据中每一个时间片上的特征提取出来，组在一起就是一个二维矩阵。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20170101112809234.png"/></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/18/Principles%20of%20fMRI%201/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/18/Principles%20of%20fMRI%201/" class="post-title-link" itemprop="url">Principles of fMRI 1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-01-18 08:46:57" itemprop="dateCreated datePublished" datetime="2023-01-18T08:46:57+08:00">2023-01-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-20 09:39:45" itemprop="dateModified" datetime="2023-01-20T09:39:45+08:00">2023-01-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="fMRI简介及数据介绍"><a href="#fMRI简介及数据介绍" class="headerlink" title="fMRI简介及数据介绍"></a>fMRI简介及数据介绍</h1><p>​        功能性磁共振成像（fMRI）：可以探测到大脑结构和动态变化，是一种非入侵技术没有副作用，在被试者完成任务时测量一系列的大脑图像，然后单个图片的测量信号的变化被用来推断任务-相关的脑活动，fMRI在一段时间内多次测量大脑某块区域。</p>
<p>​        每个大脑区域有大约100000个不同的体素，体素是三维空间内的小立方体，是fMRI分析的基石。每一个体素都有一个数字代表强度和空间位置与之对应。在连续的时间观察这个体素，就可以看到这个体素的活动强度随时间的变化，提取出关于时间的活动强度信息，关联任务或者被试的活动。通常情况下这些信号代表血氧依赖水平，测量血红蛋白与脱氧血红蛋白的比例与该组织的新陈代谢速度，间接反映神经元活动。</p>
<p>​        血流响应函数HRF，代表神经活动触发的fMRI响应。</p>
<p>​        fMRI数据分析的目标：大脑活动的mapping，决定在进行特定任务时大脑什么部位会被激活，同时处理好噪声问题。通过连接强度或者大脑活动来推测个人的感知、行为和健康状况。</p>
<h1 id="fRMI数据结构"><a href="#fRMI数据结构" class="headerlink" title="fRMI数据结构"></a>fRMI数据结构</h1><pre><code>    ## 时间分辨率与空间分布率、结构图像与功能图像
</code></pre><p>​        时间分辨率TR：由扫描一张图片所需的时间决定即参数TR决定的，决定了区分观察不同时间点上大脑变化的能力。</p>
<p>​        空间分辨率：决定了区分不同区域微小变化的能力（图片是否清晰）。</p>
<p>​        在磁场等条件一样时，fMRI扫描出的图像的时间分辨率越高/低，对应的空间分辨率就越低/高，很多时候需要根据不同的情况在两者之间找到一个平衡点。</p>
<p>​        结构图像：有着极高的空间分辨率，可以通过它区分不停的组织，而时间分辨率及其低，事实上就是一个静态的图像。</p>
<p>​        功能图像：时间分辨率相对很高，通过它可以研究不同的时间段大脑发生了什么变化，空间分辨率较低。</p>
<p>​        大脑扫面常用的方向：冠状面、矢状面、水平面。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20170101224341062.png" alt="20170101224341062"></p>
<p>​        通常来说MRI是按照水平扫描的。FOV（Filed of View）是指扫描的一块空间区域，一般只讲某一层面的视野没有第三维。</p>
<p>​        切片厚度是指一层切片的厚度，同时一层图像在扫面时会被分成很多体素，比如一层图像被分成64×64个体素，所以它的分割尺寸Matrix Size是64×64。每一个时刻每个体素都会对应一个自己的值。</p>
<h2 id="图像K空间"><a href="#图像K空间" class="headerlink" title="图像K空间"></a>图像K空间</h2><p>​        K空间的数据分布实际上是图像空间中数据的二维傅立叶变换结果，傅里叶变换实际上是将信号分解为不同频率、不同振幅的正弦波的过程。K空间中的数据点和图像空间中的数据点并不是一一对应的。一个K空间中的数据点对应了图像空间中所有数据点的一部分信息。</p>
<h2 id="信号与血流"><a href="#信号与血流" class="headerlink" title="信号与血流"></a>信号与血流</h2><p>​        MRI往往用于研究大脑的具体功能，扫出来的是功能图像，也叫做T2*权重图像，空间分辨率比较低但时间分辨率很高，可以在很短的时间内扫出一叠功能图像。fMRI实验中用到的是BOLD比例指数，BOLD指数指的是有氧血红蛋白的含量和脱氧血红蛋白含量的比值，并没有直接测量神经元的活动，是根据神经元在兴奋时所消耗的氧气量来衡量大脑活动的。有氧血红蛋白是反磁性（Diamagnetic）的，脱氧血红蛋白是顺磁性（Paramagnetic）的，这两种不同状态下的蛋白一起作用产生了局部磁场。</p>
<p>​        神经元兴奋时，血流量增加，脱氧血红蛋白含量下降，于是T2*信号随之上升，通过这个变化推断出神经元的活动。</p>
<h2 id="噪声"><a href="#噪声" class="headerlink" title="噪声"></a>噪声</h2><p>​        <a target="_blank" rel="noopener" href="https://blog.csdn.net/jinxiaonian11/article/details/54171864">参考</a></p>
<p>​        宏观神经网络：是指大脑和多个系统的模式，其空间从1cm左右到100cm左右。</p>
<p>​        功能映射地图：是指像躯体感觉这样相对粗糙的大脑区域，大约是一个大脑组织的几毫米，包含在一个体素中。</p>
<p>​        功能柱：指执行不同功能的columns。</p>
<p>​        以上三种范围内的信息可以用BOLD fMRI技术来获取。</p>
<h2 id="fMRI数据预处理"><a href="#fMRI数据预处理" class="headerlink" title="fMRI数据预处理"></a>fMRI数据预处理</h2><p>​        刚采集的原始图像数据会经过一系列的预处理步骤，主要是分辨去除伪影以及检验一些模型所需的假设是否成立。</p>
<p>​        尽量减少因为数据采集和生理学特性导致的误差，检验模型的统计假设做一些变换让数据符合这些假设，将不同的个体数据的脑区位置标准化以便进行组间分析，组间分析才具有较好的效度和灵敏度。</p>
<p>​        预处理步骤包括：可视化、去伪影、时间配准、头动校正、生理噪音校正、结构功能配准、标准化和时空间滤波。</p>
<h3 id="可视化去伪影"><a href="#可视化去伪影" class="headerlink" title="可视化去伪影"></a>可视化去伪影</h3><p>​        预处理的第一步，有些时候数据会出现一些异常陡峭的峰波或者是缓慢的偏倚，主成分分析法PCA可以用来探测这些异常的峰波。</p>
<h3 id="时间配准"><a href="#时间配准" class="headerlink" title="时间配准"></a>时间配准</h3><p>​        在扫描一次完整大脑（Brain volume）的周期（TR，Repetition time）内，会扫好几片脑片（Slice），由于一个时间点只能扫描一张脑片，如果是按照顺序一张一张扫下去的话，每张脑片之间的扫面时间点都会有区别。比如最顶端的脑片的扫描时间会相对低端的脑片延迟2s，需要做时间回归。通过插值等方法来获得三个脑片相同时间点的数据，通过与未知点相邻的已知点的信号值来预测未知点的值，通常有线性函数和三角函数。</p>
<h3 id="头动校正"><a href="#头动校正" class="headerlink" title="头动校正"></a>头动校正</h3><p>​        通常使用刚性变换把所有的图像中的脑都固定在同一个靶位置，一个刚体变换包含6个自由度，即关于X、Y、Z轴三个方向的平移以及围绕这三个轴的旋转。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/17/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84BWDSP%E6%8C%87%E4%BB%A4%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95%E4%BC%98%E5%8C%96%E7%A0%94%E7%A9%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/17/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84BWDSP%E6%8C%87%E4%BB%A4%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95%E4%BC%98%E5%8C%96%E7%A0%94%E7%A9%B6/" class="post-title-link" itemprop="url">基于图神经网络的BWDSP指令选择方法优化研究-论文阅读2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-01-17 08:55:52 / 修改时间：08:56:28" itemprop="dateCreated datePublished" datetime="2023-01-17T08:55:52+08:00">2023-01-17</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/14/Pytorch2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/14/Pytorch2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" class="post-title-link" itemprop="url">Pytorch2-梯度下降</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-01-14 21:10:48" itemprop="dateCreated datePublished" datetime="2023-01-14T21:10:48+08:00">2023-01-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-15 09:02:42" itemprop="dateModified" datetime="2023-01-15T09:02:42+08:00">2023-01-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h2><p>​        梯度为导数变化最大的值，其方向为导数变化最大的方向。</p>
<script type="math/tex; mode=display">
$\frac{\partial f}{\partial x}=\lim_{\triangle x\rightarrow\infty}\frac{f(x+\triangle x)-f(x)}{\triangle x}$</script><script type="math/tex; mode=display">
</script><p>当△x&gt;0时，对于增函数，梯度为上升方向，对于减函数，梯度为下降方向。因此需要取梯度下降的方向即梯度的反方向作为变化方向。所取梯度即为∂cost∂ω\frac{\partial cost}{\partial \omega}∂ω∂cost，更新公式为</p>
<script type="math/tex; mode=display">
\omega = \omega - \alpha \frac{\partial cost}{\partial \omega}</script><p>其中α\alphaα为学习率即所下降的步长==不能取太大==，否则会很难收敛。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/image-20230114213115420.png" alt="image-20230114213115420"></p>
<h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><ul>
<li><p>梯度下降算法容易进入局部最优解</p>
<p>但在实际问题中的局部最优点较少，或者已经基本可以当成全局最优</p>
</li>
<li><p>梯度下降算法容易陷入鞍点，即梯度为0导致参数无法更新</p>
</li>
</ul>
<h3 id="梯度公式"><a href="#梯度公式" class="headerlink" title="梯度公式"></a>梯度公式</h3><script type="math/tex; mode=display">
$cost = \frac{1}{N} \displaystyle\sum_{n=1}^{N}(\widehat y_n-y_n)^2$</script><p>可知</p>
<script type="math/tex; mode=display">
$\frac{\partial cost(w)}{\partial \omega} = \frac{\partial}{\partial w} \frac{1}{N} \displaystyle\sum_{n=1}^{N}(\widehat y_n-y_n)^2$</script><p>其中</p>
<script type="math/tex; mode=display">
$\widehat y = \omega x$</script><p>则</p>
<script type="math/tex; mode=display">
$\frac{\partial cost(w)}{\partial \omega} = \frac{\partial}{\partial w} \frac{1}{N} \displaystyle\sum_{n=1}^{N}(\omega x_n-y_n)^2$</script><script type="math/tex; mode=display">
$\frac{\partial cost(w)}{\partial \omega}
=\frac{1}{N} \displaystyle\sum_{n=1}^{N}\frac{\partial}{\partial w}(\omega x_n-y_n)^2$</script><script type="math/tex; mode=display">
$\frac{\partial cost(w)}{\partial \omega} 
=\frac{1}{N} \displaystyle\sum_{n=1}^{N}2 x_n(x_n \omega - y_n)$</script><p>即</p>
<script type="math/tex; mode=display">
$\omega = \omega - \alpha \frac{1}{N} \displaystyle\sum_{n=1}^{N}2 x_n(x_n \omega - y_n)$</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line">_cost = []</span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line"><span class="comment">#前馈计算</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"><span class="comment">#求MSE</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cost</span>(<span class="params">xs, ys</span>):</span><br><span class="line">    cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs,ys):</span><br><span class="line">        y_pred = forward(x)</span><br><span class="line">        cost += (y_pred-y) ** <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> cost/<span class="built_in">len</span>(xs)</span><br><span class="line"><span class="comment">#求梯度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">xs, ys</span>):</span><br><span class="line">    grad = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs,ys):</span><br><span class="line">        temp = forward(x)</span><br><span class="line">        grad += <span class="number">2</span>*x*(temp-y)</span><br><span class="line">    <span class="keyword">return</span> grad / <span class="built_in">len</span>(xs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">     cost_val = cost(x_data, y_data)</span><br><span class="line">     _cost.append(cost_val)</span><br><span class="line">     grad_val = gradient(x_data, y_data)</span><br><span class="line">     w -= <span class="number">0.01</span>*grad_val</span><br><span class="line">     <span class="built_in">print</span>(<span class="string">"Epoch: "</span>,epoch, <span class="string">"w = "</span>,w ,<span class="string">"loss = "</span>, cost_val)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Predict(after training)"</span>,<span class="number">4</span>,forward(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘图</span></span><br><span class="line">plt.plot(_cost,<span class="built_in">range</span>(<span class="number">100</span>))</span><br><span class="line">plt.ylabel(<span class="string">"Cost"</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epoch'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/image-20230114213749302.png" alt="image-20230114213749302"></p>
<h3 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><p>​     随机选单个样本的损失为标准。即原公式变为：</p>
<script type="math/tex; mode=display">
$\omega = \omega - \alpha \frac{\partial loss}{\partial \omega}$</script><p>其中：</p>
<script type="math/tex; mode=display">
$\frac{\partial loss_n}{\partial \omega} = 2 x_n(x_n \omega - y_n)$</script><p>​        随机梯度下降又可能跨越鞍点，神经网络经常使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机梯度下降</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line">_cost = []</span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line"><span class="comment">#前馈计算</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"><span class="comment">#求单个loss</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred-y) ** <span class="number">2</span></span><br><span class="line"><span class="comment">#求梯度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*x*(x*w-y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Predict(after training)"</span>,<span class="number">4</span>,forward(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        grad=gradient(x,y) </span><br><span class="line">        w -= <span class="number">0.01</span>*grad</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"\tgrad:  "</span>,x,y,grad)</span><br><span class="line">        l = loss(x,y)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"progress: "</span>,epoch,<span class="string">"w="</span>,w,<span class="string">"loss="</span>,l)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Predict(after training)"</span>,<span class="number">4</span>,forward(<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<h3 id="批量梯度下降（mini-batch）"><a href="#批量梯度下降（mini-batch）" class="headerlink" title="批量梯度下降（mini-batch）"></a>批量梯度下降（mini-batch）</h3><p>​        普通的梯度下降算法利用数据整体，不容易避免鞍点，算法性能上欠佳，但算法效率高，可以并行运算。随机梯度下降需要利用每个的单个数据，虽然算法性能上良好，但计算过程往往相扣无法将样本抽离开，因此算法效率低，时间复杂度高。</p>
<p>​        综上可采取折中方法，即==批量梯度下降算法==。将若干个样本分为一组，纪录一组的梯度用以代替随机梯度下降中的单个样本。该方法常用，也是默认接口。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/14/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/14/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81/" class="post-title-link" itemprop="url">Pytorch1-线性模型代码</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-01-14 20:27:08 / 修改时间：21:04:11" itemprop="dateCreated datePublished" datetime="2023-01-14T20:27:08+08:00">2023-01-14</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"><span class="comment">#前馈计算</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"><span class="comment">#求loss</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred-y)*(y_pred-y)</span><br><span class="line"></span><br><span class="line">w_list = []</span><br><span class="line">mse_list = []</span><br><span class="line"><span class="comment">#从0.0一直到4.1以0.1为间隔进行w的取样</span></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0.0</span>,<span class="number">4.1</span>,<span class="number">0.1</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;w=&quot;</span>, w)</span><br><span class="line">    l_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_val,y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        y_pred_val = forward(x_val)</span><br><span class="line">        loss_val = loss(x_val,y_val)</span><br><span class="line">        l_sum += loss_val</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span>,x_val,y_val,y_pred_val,loss_val)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;MSE=&quot;</span>,l_sum/<span class="number">3</span>)</span><br><span class="line">    w_list.append(w)</span><br><span class="line">    mse_list.append(l_sum/<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘图</span></span><br><span class="line">plt.plot(w_list,mse_list)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Loss&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/14/%E9%9D%A2%E5%90%91%E8%87%AA%E9%97%AD%E7%97%87%E8%BE%85%E5%8A%A9%E8%AF%8A%E6%96%AD%E7%9A%84%E8%81%94%E5%90%88%E7%BB%84%E7%A8%80%E7%96%8FTSK%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/14/%E9%9D%A2%E5%90%91%E8%87%AA%E9%97%AD%E7%97%87%E8%BE%85%E5%8A%A9%E8%AF%8A%E6%96%AD%E7%9A%84%E8%81%94%E5%90%88%E7%BB%84%E7%A8%80%E7%96%8FTSK%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB1/" class="post-title-link" itemprop="url">面向自闭症辅助诊断的联合组稀疏TSK建模方法(论文阅读1)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-01-14 11:07:58" itemprop="dateCreated datePublished" datetime="2023-01-14T11:07:58+08:00">2023-01-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-15 09:02:29" itemprop="dateModified" datetime="2023-01-15T09:02:29+08:00">2023-01-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>​    静息态功能磁共振成像（resting-statefunctional magnetic resonance imaging，rs-fMRI）技术能够在无创伤、无辐射条件下，通过检测血氧水平获得高分辨率图像来体现大脑活动的异常。自闭症的形成与大脑的形态结构变化密切相关，患者脑功能连接方面存在着不同脑区之间近距离连接过度、远距离连接不足等问题。</p>
<p>​    TSK 模糊系统是一种高效的模糊推理系统，对解决不确定性问题具有很好的针对性。其核心思想是通过对训练数据的输入/输出集合进行划分来提取“if-then”模糊规则，在此基础上进行模糊规则后件参数的学习来挖掘输入数据和输出数据之间的映射关系。联合组稀疏TSK模糊系统建模方法，基于TSK模糊系统理论框架，结合特征之间的关联信息学习新的权重系数，使用一种全新的方式来构造不同模糊规则后件参数之间的联合组稀疏正则化项，引导规则内特征和规则间特征的联合选择，从而降低不确定性。</p>
<h2 id="2-数据处理"><a href="#2-数据处理" class="headerlink" title="2 数据处理"></a>2 数据处理</h2><h3 id="2-1-数据预处理"><a href="#2-1-数据预处理" class="headerlink" title="2.1 数据预处理"></a>2.1 数据预处理</h3><h3 id="2-2-面向rs-fMRI数据的特征提取"><a href="#2-2-面向rs-fMRI数据的特征提取" class="headerlink" title="2.2 面向rs-fMRI数据的特征提取"></a>2.2 面向rs-fMRI数据的特征提取</h3><p>​    计算每个样本个脑区之间的Pearson相关系数，得到功能连接矩阵，该矩阵表示脑区之间的线性相关程度，具有对称性。</p>
<p>​    在训练集中选出相关系数最大的P个特征组成新的训练集，根据索引更新对应的验验证集的测试集，</p>
<h2 id="3-特征关联诱导联合组稀疏TSK模糊系统"><a href="#3-特征关联诱导联合组稀疏TSK模糊系统" class="headerlink" title="3 特征关联诱导联合组稀疏TSK模糊系统"></a>3 特征关联诱导联合组稀疏TSK模糊系统</h2><h3 id="3-1-TSK模糊系统"><a href="#3-1-TSK模糊系统" class="headerlink" title="3.1 TSK模糊系统"></a>3.1 TSK模糊系统</h3><h3 id="3-2-规则间联合组稀疏学习"><a href="#3-2-规则间联合组稀疏学习" class="headerlink" title="3.2 规则间联合组稀疏学习"></a>3.2 规则间联合组稀疏学习</h3><h3 id="3-3-权重系数学习"><a href="#3-3-权重系数学习" class="headerlink" title="3.3 权重系数学习"></a>3.3 权重系数学习</h3><h3 id="3-4-目标函数优化"><a href="#3-4-目标函数优化" class="headerlink" title="3.4 目标函数优化"></a>3.4 目标函数优化</h3><h2 id="4-实验设置"><a href="#4-实验设置" class="headerlink" title="4 实验设置"></a>4 实验设置</h2><p>​    常用指标为敏感度SEN(sensitivity)和特异性SPE(apecificity)，SEN越高，漏诊率越低，确诊病人的可能性越大；SPE越高，误诊率越低。分辨正常人的能力越低。ROC曲线的面积越大，AUC值越大，，所对应的算法的诊断性能越好。</p>
<p>​    在训练集上训练模型，在验证集上进行网格参数寻优，在测试集上评估分类性能。采用阈值为0.5的sigmoid函数实现分类，即大于等于0.5时为正例，小于0.5时为负例。</p>
<p>​    实验表明，联合组稀疏非线性模糊分类方法JGSL-TSK能够有效改进分类模型在辅助诊断自闭症上的性能。模糊推理系统针对解决不确定性问题具有更好的非线性逼近能力。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/image-20230114112041004.png" alt=""></p>
<p>​    JGSL-TSK在进行特征提取的基础上，不仅通过稀疏化精度矩阵提取规则内特征之间的相关信息，而且在构造分类模型的过程中，引入L1、L2正则化实现规则间的特征选择，从而有效地降低噪声影响，更好地利用特征间的关联信息提高确诊功能，且收敛速度快，在迭代5~10次左右目标函数便趋于稳定，实用性较好。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/13/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/13/hello-world/" class="post-title-link" itemprop="url">第一篇Hexo</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-01-13 13:17:46 / 修改时间：21:04:23" itemprop="dateCreated datePublished" datetime="2023-01-13T13:17:46+08:00">2023-01-13</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">yi-yi1</p>
  <div class="site-description" itemprop="description">种一棵树最好的时间是现在</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yi-yi1</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
