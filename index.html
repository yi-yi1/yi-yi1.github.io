<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="种一棵树最好的时间是现在">
<meta property="og:type" content="website">
<meta property="og:title" content="不就熬个夜">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="不就熬个夜">
<meta property="og:description" content="种一棵树最好的时间是现在">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="yi-yi1">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>不就熬个夜</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">不就熬个夜</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/24/Pytorch6-%E5%A4%9A%E7%BB%B4%E5%BA%A6%E8%BE%93%E5%85%A5%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/24/Pytorch6-%E5%A4%9A%E7%BB%B4%E5%BA%A6%E8%BE%93%E5%85%A5%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">Pytorch6-多维度输入问题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-01-24 16:23:13 / 修改时间：16:39:02" itemprop="dateCreated datePublished" datetime="2023-01-24T16:23:13+08:00">2023-01-24</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>​        对于单维度逻辑回归模型有：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="17.584ex" height="2.587ex" role="img" focusable="false" viewBox="0 -893.3 7772.2 1143.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(300.6,16) translate(-250 0)"><path data-c="2C6" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g><g data-mml-node="TeXAtom" transform="translate(523,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(734,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1644.9,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2700.6,0)"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(3271.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(3660.6,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(605,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(734,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g><g data-mml-node="mi" transform="translate(5109.7,0)"><path data-c="1D714" d="M495 384Q495 406 514 424T555 443Q574 443 589 425T604 364Q604 334 592 278T555 155T483 38T377 -11Q297 -11 267 66Q266 68 260 61Q201 -11 125 -11Q15 -11 15 139Q15 230 56 325T123 434Q135 441 147 436Q160 429 160 418Q160 406 140 379T94 306T62 208Q61 202 61 187Q61 124 85 100T143 76Q201 76 245 129L253 137V156Q258 297 317 297Q348 297 348 261Q348 243 338 213T318 158L308 135Q309 133 310 129T318 115T334 97T358 83T393 76Q456 76 501 148T546 274Q546 305 533 325T508 357T495 384Z"></path></g><g data-mml-node="mo" transform="translate(5953.9,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(6954.2,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(7383.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>，其中的<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="3.278ex" height="2.046ex" role="img" focusable="false" viewBox="0 -893.3 1449.1 904.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(605,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(734,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></g></g></svg></mjx-container>表示第i个样本的维度，对于多维度，输入要变成8个维度，因此模型应当变为</p>
<script type="math/tex; mode=display">
\widehat y^{(i)} = \sigma(\sum _{n=1}^8 x^{(i)}_n \omega _n+b))</script><p>其中的<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.332ex;" xmlns="http://www.w3.org/2000/svg" width="3.278ex" height="2.732ex" role="img" focusable="false" viewBox="0 -1060.7 1449.1 1207.4"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(605,530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(734,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mi" transform="translate(605,-138.9) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>，表示第i个样本的第n个维度。由于在实际代码运算中是以矩阵进行计算的，因此其中：</p>
<script type="math/tex; mode=display">
\sum _{n=1}^8 x^{(i)}_n \omega _n = 
\begin{bmatrix}
{x_1^{(i)}}&{\cdots}&{x_8^{(i)}}
\end{bmatrix}
\begin{bmatrix}
{w_1}\\
{\vdots}\\
{w_8}
\end{bmatrix}</script><p>则原式可以表示成</p>
<script type="math/tex; mode=display">
\widehat y^{(i)} = \sigma(
\begin{bmatrix}
{x_1^{(i)}}&{\cdots}&{x_8^{(i)}}
\end{bmatrix}
\begin{bmatrix}
{w_1}\\
{\vdots}\\
{w_8}
\end{bmatrix}+b)\\
=\sigma(z^{(i)})</script><h2 id="网络的增加"><a href="#网络的增加" class="headerlink" title="网络的增加"></a>网络的增加</h2><p>​        矩阵实质上是用于==空间==的函数。</p>
<p>​        只需要将Linear()中的参数改成下面代码，即可完成从8维输入到1维输出的过程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.linear = torch.nn.Linear(<span class="number">8</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230124163451.png"></p>
<p>也可以将输出的部分转化为其他维度，来实现分布的维度下降，比如8维转6维，6维转4维，4维转1维，由此来可以增加网络层数和网络复杂度，对网络结构先增后减也是可以的。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230124163818.png"></p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#读取文件，一般GPU只支持32位浮点数</span></span><br><span class="line">xy = np.loadtxt(<span class="string">'diabetes.csv'</span>, delimiter=<span class="string">','</span>, dtype = np.float32)</span><br><span class="line"><span class="comment">#-1行-1列不取</span></span><br><span class="line">x_data = torch.from_numpy(xy[:-<span class="number">1</span>, :-<span class="number">1</span>])</span><br><span class="line"><span class="comment">#单取-1列作为矩阵</span></span><br><span class="line">y_data = torch.from_numpy(xy[:-<span class="number">1</span>, [-<span class="number">1</span>]])</span><br><span class="line"><span class="comment">#取-1行的测试集部分</span></span><br><span class="line">test_data = torch.from_numpy(xy[[-<span class="number">1</span>], :-<span class="number">1</span>])</span><br><span class="line">pred_test = torch.from_numpy(xy[[-<span class="number">1</span>],[-<span class="number">1</span>]])</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>, <span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>, <span class="number">4</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        self.sigmoid = torch.nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.sigmoid(self.linear1(x))</span><br><span class="line">        x = self.sigmoid(self.linear2(x))</span><br><span class="line">        x = self.sigmoid(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = Model()</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.BCELoss(size_average=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment">#Forward 并非mini-batch的设计，只是mini-batch的风格</span></span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Backward</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Update</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"test_pred = "</span>, model(test_data).item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"infact_pred = "</span>, pred_test.item())</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/23/Pytorch5-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/23/Pytorch5-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">Pytorch5-分类问题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-01-23 20:39:58" itemprop="dateCreated datePublished" datetime="2023-01-23T20:39:58+08:00">2023-01-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-24 16:25:08" itemprop="dateModified" datetime="2023-01-24T16:25:08+08:00">2023-01-24</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>​        分类问题中，实际上是对概率的计算与比较，而非类别之间的数值比较。</p>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><h2 id="二分类问题"><a href="#二分类问题" class="headerlink" title="二分类问题"></a>二分类问题</h2><p>​        二分类问题是非0即1的问题，由于隐藏条件的限制，有：</p>
<script type="math/tex; mode=display">
P(\widehat y=1)+P(\widehat y=0) = 1</script><p>对于二分类问题结果的预测，仅需要计算在0或1的条件下即可得到答案，通常计算1的概率。</p>
<h2 id="逻辑函数"><a href="#逻辑函数" class="headerlink" title="逻辑函数"></a>逻辑函数</h2><p>​        在原先的回归问题中，所用的模型为</p>
<script type="math/tex; mode=display">
\widehat y = \omega x+b</script><p>此时<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="5.592ex" height="2.296ex" role="img" focusable="false" viewBox="0 -810 2471.6 1015"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(300.6,16) translate(-250 0)"><path data-c="2C6" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g><g data-mml-node="mo" transform="translate(767.8,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mi" transform="translate(1712.6,0)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g></g></g></svg></mjx-container>，但当问题为分类问题时，所求结果的值域应当发生变化，变为一个即<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="8.401ex" height="2.398ex" role="img" focusable="false" viewBox="0 -810 3713.2 1060"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(300.6,16) translate(-250 0)"><path data-c="2C6" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g><g data-mml-node="mo" transform="translate(767.8,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mo" transform="translate(1712.6,0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mn" transform="translate(1990.6,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(2490.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(2935.2,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(3435.2,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container>，因此需要引入逻辑函数(sigmod)来实现。</p>
<script type="math/tex; mode=display">
\sigma(x) = \frac{1}{1+e^{-x}}</script><p>本函数原名为logistics函数，属于sigmoid类函数，由于其特性优异，代码中sigmoid函数就指的是本函数，图像为</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230123205841.png"></p>
<p>特点：</p>
<ol>
<li>函数值在0到1之间变化明显，即导数大</li>
<li>在趋于0和1处函数逐渐平滑，即导数小</li>
<li>函数为饱和函数</li>
<li>单调函数</li>
</ol>
<p>除此之外，还有其他类的sigmoid函数。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230123211101.png"></p>
<h2 id="模型变换"><a href="#模型变换" class="headerlink" title="模型变换"></a>模型变换</h2><p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230124160921.png"></p>
<h3 id="Loss变化"><a href="#Loss变化" class="headerlink" title="Loss变化"></a>Loss变化</h3><p>​        原先是计算两个标量数值间的差距，也就是数轴上的距离。计算两个概率之间的差异，需要利用到交叉熵的理论。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230124161037.png"></p>
<p>​        如下图所示，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.109ex" height="2.296ex" role="img" focusable="false" viewBox="0 -810 490 1015"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(300.6,16) translate(-250 0)"><path data-c="2C6" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g></g></g></svg></mjx-container>与y越接近，BCE Loss越小</p>
<h2 id="整体代码"><a href="#整体代码" class="headerlink" title="整体代码"></a>整体代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>],[<span class="number">2.0</span>],[<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">0.0</span>],[<span class="number">0.0</span>],[<span class="number">1.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#改用LogisticRegressionModel 同样继承于Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LogisticRegressionModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LogisticRegressionModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment">#对原先的linear结果进行sigmod激活</span></span><br><span class="line">        y_pred = F.sigmoid(self.linear(x))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line">model = LogisticRegressionModel()</span><br><span class="line"></span><br><span class="line"><span class="comment">#构造的criterion对象所接受的参数为（y',y） 改用BCE</span></span><br><span class="line">criterion = torch.nn.BCELoss(size_average=<span class="literal">False</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch,loss)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'w = '</span>, model.linear.weight.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'b = '</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'y_pred = '</span>,y_test.data)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/23/Pytorch4-%E5%9F%BA%E4%BA%8EPytorch%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/23/Pytorch4-%E5%9F%BA%E4%BA%8EPytorch%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" class="post-title-link" itemprop="url">Pytorch4-基于Pytorch的线性模型</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-01-23 14:05:44 / 修改时间：20:42:19" itemprop="dateCreated datePublished" datetime="2023-01-23T14:05:44+08:00">2023-01-23</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="利用pytorch进行深度学习"><a href="#利用pytorch进行深度学习" class="headerlink" title="利用pytorch进行深度学习"></a>利用pytorch进行深度学习</h1><ul>
<li>准备数据集</li>
<li>设计用于计算最终结果的模型</li>
<li>构造损失函数及优化器</li>
<li>设计循环周期Training cycle——==前馈、反馈、更新==</li>
</ul>
<p>​        x,y∈R，在pytorch中，若使用mini-batch的算法，一次性求出一个批量的y，则需要x以及y作为矩阵参与计算，此时利用其广播机制，可以将原标量参数ω扩写为同维的矩阵[ω]，参与运算而不改变其Tensor的性质，对于矩阵，行表示样本，列表示特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#数据作为矩阵参与Tensor计算</span></span><br><span class="line">x_data = torch.Tensor([<span class="number">1.0</span>],[<span class="number">2.0</span>],[<span class="number">3.0</span>])</span><br><span class="line">y_data = torch.Tensor([<span class="number">2.0</span>],[<span class="number">4.0</span>],[<span class="number">6.0</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>​        ω以及b是需要反复训练确定的，在设计时，需要率先设计出此二者的维度。只要确定了y以及x的维度，就可以确定ω以及b的维度大小。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230123192843.png"></p>
<p>​        由于前边的计算过程都是针对矩阵的，因此最后的loss也是矩阵，但由于进行反向传播调整参数，因此loss应当是个标量，因此要对矩阵[loss]内每个量求和均值(MSE)</p>
<script type="math/tex; mode=display">
loss = \frac{1}{N}\Sigma
\begin{bmatrix}
{loss_1}\\
{\vdots}\\
{loss_n}\\
\end{bmatrix}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#固定继承于Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="comment">#构造函数初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment">#调用父类的init</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        <span class="comment">#Linear对象包括weight(w)以及bias(b)两个成员张量</span></span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#前馈函数forward，对父类函数中的overwrite</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment">#调用linear中的call()，以利用父类forward()计算wx+b</span></span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line">    <span class="comment">#反馈函数backward由module自动根据计算图生成</span></span><br><span class="line">model = LinearModel()</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230123195256.png"></p>
<h1 id="构造loss和optimizer"><a href="#构造loss和optimizer" class="headerlink" title="构造loss和optimizer"></a>构造loss和optimizer</h1><h2 id="loss计算"><a href="#loss计算" class="headerlink" title="loss计算"></a>loss计算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>​        其中MSELoss也是继承于nn.Modules的。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230123195512.png"></p>
<h2 id="optimizer计算"><a href="#optimizer计算" class="headerlink" title="optimizer计算"></a>optimizer计算</h2><p>​        优化器并不构建计算图，生产的优化器对象可以直接对整个模型进行优化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#model.parameters()用于检查模型中所能进行优化的张量</span></span><br><span class="line"><span class="comment">#learningrate(lr)表学习率，可以统一也可以不统一</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>
<h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><ol>
<li>前馈计算预测值与损失函数</li>
<li>梯度清零并进行backward</li>
<li>更新参数</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="comment">#前馈计算y_pred</span></span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    <span class="comment">#前馈计算损失loss</span></span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="comment">#打印调用loss时，会自动调用内部__str__()函数，避免产生计算图</span></span><br><span class="line">    <span class="built_in">print</span>(epoch,loss)</span><br><span class="line">    <span class="comment">#梯度清零</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment">#梯度反向传播，计算图清除</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment">#根据传播的梯度以及学习率更新参数</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<h2 id="测试模型"><a href="#测试模型" class="headerlink" title="测试模型"></a>测试模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">#Output</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'w = '</span>, model.linear.weight.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'b = '</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line"><span class="comment">#TestModel</span></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'y_pred = '</span>,y_test.data)</span><br></pre></td></tr></table></figure>
<h1 id="整体代码"><a href="#整体代码" class="headerlink" title="整体代码"></a>整体代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#数据作为矩阵参与Tensor计算</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>],[<span class="number">2.0</span>],[<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">2.0</span>],[<span class="number">4.0</span>],[<span class="number">6.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#固定继承于Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="comment">#构造函数初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment">#调用父类的init</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        <span class="comment">#Linear对象包括weight(w)以及bias(b)两个成员张量</span></span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#前馈函数forward，对父类函数中的overwrite</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment">#调用linear中的call()，以利用父类forward()计算wx+b</span></span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line">    <span class="comment">#反馈函数backward由module自动根据计算图生成</span></span><br><span class="line">model = LinearModel()</span><br><span class="line"></span><br><span class="line"><span class="comment">#构造的criterion对象所接受的参数为（y',y）</span></span><br><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#model.parameters()用于检查模型中所能进行优化的张量</span></span><br><span class="line"><span class="comment">#learningrate(lr)表学习率，可以统一也可以不统一</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment">#前馈计算y_pred</span></span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    <span class="comment">#前馈计算损失loss</span></span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="comment">#打印调用loss时，会自动调用内部__str__()函数，避免产生计算图</span></span><br><span class="line">    <span class="built_in">print</span>(epoch,loss)</span><br><span class="line">    <span class="comment">#梯度清零</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment">#梯度反向传播，计算图清除</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment">#根据传播的梯度以及学习率更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"> <span class="comment">#Output</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'w = '</span>, model.linear.weight.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'b = '</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line"><span class="comment">#TestModel</span></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'y_pred = '</span>,y_test.data)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/21/Pytorch3-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/21/Pytorch3-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/" class="post-title-link" itemprop="url">Pytorch3-反向传播</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-01-21 09:53:45 / 修改时间：11:50:07" itemprop="dateCreated datePublished" datetime="2023-01-21T09:53:45+08:00">2023-01-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>​        在线性模型中，</p>
<script type="math/tex; mode=display">
\widehat y = \omega x</script><p>如果</p>
<p>以神经网络的视角带入来看，则x为输入层，即input层，ω为<strong>权重</strong>。在神经网络中，通常将权重以及乘法计算操作的部分合并看做一个神经元（层）。而神经网络的训练过程即为更新权重的过程，其更新的情况依赖于</p>
<script type="math/tex; mode=display">
\frac{\partial loss}{\partial \omega}</script><p>,而并非</p>
<script type="math/tex; mode=display">
\frac{\partial \widehat y}{\partial \omega}</script><p>​        神经网络视角下的线性模型：</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230121104631.png"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">x~i~</td>
<td style="text-align:center">输入层的第i个结点</td>
</tr>
<tr>
<td style="text-align:center">h~ij~</td>
<td style="text-align:center">第i层隐含层的第j个结点</td>
</tr>
<tr>
<td style="text-align:center">o~i~</td>
<td style="text-align:center">输出层的第i个结点</td>
</tr>
<tr>
<td style="text-align:center">ω~x1~^mn^</td>
<td style="text-align:center">输入层的第m个结点与隐含层的第n个结点之间的权重</td>
</tr>
<tr>
<td style="text-align:center">ω~ij~^mn^</td>
<td style="text-align:center">隐含层第i层的第m个结点与第j层的第n个结点之间的权重</td>
</tr>
<tr>
<td style="text-align:center">ω~ko~^mn^</td>
<td style="text-align:center">隐含层最后一层（第k层）的第m个结点与输出层第n个结点之间的权重</td>
</tr>
</tbody>
</table>
</div>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230121110246.png"></p>
<p>​        上图中，输入层与隐藏层第一层之间有5×6=30个权重，而隐藏层第一层和隐藏层第二层之间由6×7=42个权重</p>
<h2 id="计算图中的神经网络"><a href="#计算图中的神经网络" class="headerlink" title="计算图中的神经网络"></a>计算图中的神经网络</h2><p>​        在计算图中，绿色的模块为计算模块，可以在计算过程中求导。MM为矩阵乘法，ADD为加法。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230121111341.png"></p>
<p>对于上图左式，可以化简得到如下公式：</p>
<script type="math/tex; mode=display">
\widehat y = W_2(W_1X+b_1)+b_2=W_2W_1X+(W_2b_1+b_2)=WX+b</script><p>也就是说，在这个结构下单纯的增加层数，并不能怎加神经网络的复杂程度，因为</p>
<p>最后都可以化简为一个单一的神经网络。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230121112009.png"></p>
<h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><p>​        在每层网结构中，增加一个非线性的==变换函数即激活函数==</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230121112314.png"></p>
<h1 id="反向传播过程"><a href="#反向传播过程" class="headerlink" title="反向传播过程"></a>反向传播过程</h1><h2 id="前馈计算"><a href="#前馈计算" class="headerlink" title="前馈计算"></a>前馈计算</h2><p>​        在某一处神经元，输入的x与ω经过函数f(x,ω)的计算，可以获得输出值z，并继续向前以得到损失值loss。在f(x,ω)的计算模块中会计算导数</p>
<script type="math/tex; mode=display">
\frac{\partial z}{\partial x}和\frac{\partial z}{\partial \omega}</script><p>并将其保存下来，在pytorch中，这样的值保存在变量x以及ω中。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230121113244.png"></p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>​        由于求导的链式法则，求得loss以后，前面的神经元会将</p>
<script type="math/tex; mode=display">
\frac{\partial loss}{\partial z}</script><p>的值反向传播给原先的神经元，在计算单元f(x,ω)中，将得到的</p>
<script type="math/tex; mode=display">
\frac{\partial loss}{\partial x}</script><p>与之前存储的导数相乘，即可得到损失值对于权重以及输入层的导数，即</p>
<script type="math/tex; mode=display">
\frac{\partial loss}{\partial x}和\frac{\partial loss}{\partial \omega}</script><p>基于该梯度才进行权重的调整。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230121114218.png"></p>
<h1 id="Pytorch中的前馈与反馈"><a href="#Pytorch中的前馈与反馈" class="headerlink" title="Pytorch中的前馈与反馈"></a>Pytorch中的前馈与反馈</h1><p>​        利用Pytorch框架进行深度学习，最主要的是==构建计算图==</p>
<h2 id="Tensor张量"><a href="#Tensor张量" class="headerlink" title="Tensor张量"></a>Tensor张量</h2><p>​        Tensor中重要的两个成员，data用于保存权重本身的值ω，grad用于保存损失函数对权重的导数，grad本身也是个张量。对张量进行的计算操作，都是建立计算图的过程。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20230121114718.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#赋予tensor中的data</span></span><br><span class="line">w = torch.Tensor([<span class="number">1.0</span>])</span><br><span class="line"><span class="comment">#设定需要计算梯度grad</span></span><br><span class="line">w.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#模型y=x*w 建立计算图</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    w为Tensor类型</span></span><br><span class="line"><span class="string">    x强制转换为Tensor类型</span></span><br><span class="line"><span class="string">    通过这样的方式建立计算图</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (<span class="string">"predict  (before training)"</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        <span class="comment">#创建新的计算图</span></span><br><span class="line">        l = loss(x,y)</span><br><span class="line">        <span class="comment">#进行反馈计算，此时才开始求梯度，此后计算图进行释放</span></span><br><span class="line">        l.backward()</span><br><span class="line">        <span class="comment">#grad.item()取grad中的值变成标量</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'\tgrad:'</span>,x, y, w.grad.item())</span><br><span class="line">        <span class="comment">#单纯的数值计算要利用data，而不能用张量，否则会在内部创建新的计算图</span></span><br><span class="line">        w.data = w.data - <span class="number">0.01</span> * w.grad.data</span><br><span class="line">        <span class="comment">#把权重梯度里的数据清零</span></span><br><span class="line">        w.grad.data.zero_()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"progress:"</span>,epoch, l.item())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"predict (after training)"</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/20/Nilearn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/20/Nilearn/" class="post-title-link" itemprop="url">Nilearn</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-01-20 10:55:48" itemprop="dateCreated datePublished" datetime="2023-01-20T10:55:48+08:00">2023-01-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-21 10:48:29" itemprop="dateModified" datetime="2023-01-21T10:48:29+08:00">2023-01-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Nilearn-库介绍"><a href="#Nilearn-库介绍" class="headerlink" title="Nilearn 库介绍"></a>Nilearn 库介绍</h1><p>​        Nilearn 是一个将机器学习、模式识别、多变量分析等技术应用于神经影像数据的应用中，能完成多体素模式分析、解码、模型预测、构造功能连接、脑区分割、构造连接体等功能。一般用于处理功能磁共振图像（fMRI）、静息状态，或者基于体素的形态学分析。Nilearn的价值体现在特定领域特定工程的构造，将神经影像数据表达为非常合适于统计学习的特征矩阵。</p>
<h1 id="从fMRI数据到时间序列"><a href="#从fMRI数据到时间序列" class="headerlink" title="从fMRI数据到时间序列"></a>从fMRI数据到时间序列</h1><p>​        fMRI数据是4维的，包含三维的空间序列和一维的时间，在实际应用中，更多的是利用大脑图像时间序列做研究，无法直接使用fMRI数据做相关研究，需要对原始数据做一些数据预处理和变换。</p>
<h2 id="mask"><a href="#mask" class="headerlink" title="mask"></a>mask</h2><p>​        第一步所做的都是把四维的fMRI数据转换为二维矩阵，这个过程称为mask，就是提取能利用的特征，通过mask得到的二维矩阵包含一维的时间和一维的特征，将fMRI数据中每一个时间片上的特征提取出来，组在一起就是一个二维矩阵。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20170101112809234.png"/></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/18/Principles%20of%20fMRI%201/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/18/Principles%20of%20fMRI%201/" class="post-title-link" itemprop="url">Principles of fMRI 1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-01-18 08:46:57" itemprop="dateCreated datePublished" datetime="2023-01-18T08:46:57+08:00">2023-01-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-20 09:39:45" itemprop="dateModified" datetime="2023-01-20T09:39:45+08:00">2023-01-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="fMRI简介及数据介绍"><a href="#fMRI简介及数据介绍" class="headerlink" title="fMRI简介及数据介绍"></a>fMRI简介及数据介绍</h1><p>​        功能性磁共振成像（fMRI）：可以探测到大脑结构和动态变化，是一种非入侵技术没有副作用，在被试者完成任务时测量一系列的大脑图像，然后单个图片的测量信号的变化被用来推断任务-相关的脑活动，fMRI在一段时间内多次测量大脑某块区域。</p>
<p>​        每个大脑区域有大约100000个不同的体素，体素是三维空间内的小立方体，是fMRI分析的基石。每一个体素都有一个数字代表强度和空间位置与之对应。在连续的时间观察这个体素，就可以看到这个体素的活动强度随时间的变化，提取出关于时间的活动强度信息，关联任务或者被试的活动。通常情况下这些信号代表血氧依赖水平，测量血红蛋白与脱氧血红蛋白的比例与该组织的新陈代谢速度，间接反映神经元活动。</p>
<p>​        血流响应函数HRF，代表神经活动触发的fMRI响应。</p>
<p>​        fMRI数据分析的目标：大脑活动的mapping，决定在进行特定任务时大脑什么部位会被激活，同时处理好噪声问题。通过连接强度或者大脑活动来推测个人的感知、行为和健康状况。</p>
<h1 id="fRMI数据结构"><a href="#fRMI数据结构" class="headerlink" title="fRMI数据结构"></a>fRMI数据结构</h1><pre><code>    ## 时间分辨率与空间分布率、结构图像与功能图像
</code></pre><p>​        时间分辨率TR：由扫描一张图片所需的时间决定即参数TR决定的，决定了区分观察不同时间点上大脑变化的能力。</p>
<p>​        空间分辨率：决定了区分不同区域微小变化的能力（图片是否清晰）。</p>
<p>​        在磁场等条件一样时，fMRI扫描出的图像的时间分辨率越高/低，对应的空间分辨率就越低/高，很多时候需要根据不同的情况在两者之间找到一个平衡点。</p>
<p>​        结构图像：有着极高的空间分辨率，可以通过它区分不停的组织，而时间分辨率及其低，事实上就是一个静态的图像。</p>
<p>​        功能图像：时间分辨率相对很高，通过它可以研究不同的时间段大脑发生了什么变化，空间分辨率较低。</p>
<p>​        大脑扫面常用的方向：冠状面、矢状面、水平面。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/20170101224341062.png" alt="20170101224341062"></p>
<p>​        通常来说MRI是按照水平扫描的。FOV（Filed of View）是指扫描的一块空间区域，一般只讲某一层面的视野没有第三维。</p>
<p>​        切片厚度是指一层切片的厚度，同时一层图像在扫面时会被分成很多体素，比如一层图像被分成64×64个体素，所以它的分割尺寸Matrix Size是64×64。每一个时刻每个体素都会对应一个自己的值。</p>
<h2 id="图像K空间"><a href="#图像K空间" class="headerlink" title="图像K空间"></a>图像K空间</h2><p>​        K空间的数据分布实际上是图像空间中数据的二维傅立叶变换结果，傅里叶变换实际上是将信号分解为不同频率、不同振幅的正弦波的过程。K空间中的数据点和图像空间中的数据点并不是一一对应的。一个K空间中的数据点对应了图像空间中所有数据点的一部分信息。</p>
<h2 id="信号与血流"><a href="#信号与血流" class="headerlink" title="信号与血流"></a>信号与血流</h2><p>​        MRI往往用于研究大脑的具体功能，扫出来的是功能图像，也叫做T2*权重图像，空间分辨率比较低但时间分辨率很高，可以在很短的时间内扫出一叠功能图像。fMRI实验中用到的是BOLD比例指数，BOLD指数指的是有氧血红蛋白的含量和脱氧血红蛋白含量的比值，并没有直接测量神经元的活动，是根据神经元在兴奋时所消耗的氧气量来衡量大脑活动的。有氧血红蛋白是反磁性（Diamagnetic）的，脱氧血红蛋白是顺磁性（Paramagnetic）的，这两种不同状态下的蛋白一起作用产生了局部磁场。</p>
<p>​        神经元兴奋时，血流量增加，脱氧血红蛋白含量下降，于是T2*信号随之上升，通过这个变化推断出神经元的活动。</p>
<h2 id="噪声"><a href="#噪声" class="headerlink" title="噪声"></a>噪声</h2><p>​        <a target="_blank" rel="noopener" href="https://blog.csdn.net/jinxiaonian11/article/details/54171864">参考</a></p>
<p>​        宏观神经网络：是指大脑和多个系统的模式，其空间从1cm左右到100cm左右。</p>
<p>​        功能映射地图：是指像躯体感觉这样相对粗糙的大脑区域，大约是一个大脑组织的几毫米，包含在一个体素中。</p>
<p>​        功能柱：指执行不同功能的columns。</p>
<p>​        以上三种范围内的信息可以用BOLD fMRI技术来获取。</p>
<h2 id="fMRI数据预处理"><a href="#fMRI数据预处理" class="headerlink" title="fMRI数据预处理"></a>fMRI数据预处理</h2><p>​        刚采集的原始图像数据会经过一系列的预处理步骤，主要是分辨去除伪影以及检验一些模型所需的假设是否成立。</p>
<p>​        尽量减少因为数据采集和生理学特性导致的误差，检验模型的统计假设做一些变换让数据符合这些假设，将不同的个体数据的脑区位置标准化以便进行组间分析，组间分析才具有较好的效度和灵敏度。</p>
<p>​        预处理步骤包括：可视化、去伪影、时间配准、头动校正、生理噪音校正、结构功能配准、标准化和时空间滤波。</p>
<h3 id="可视化去伪影"><a href="#可视化去伪影" class="headerlink" title="可视化去伪影"></a>可视化去伪影</h3><p>​        预处理的第一步，有些时候数据会出现一些异常陡峭的峰波或者是缓慢的偏倚，主成分分析法PCA可以用来探测这些异常的峰波。</p>
<h3 id="时间配准"><a href="#时间配准" class="headerlink" title="时间配准"></a>时间配准</h3><p>​        在扫描一次完整大脑（Brain volume）的周期（TR，Repetition time）内，会扫好几片脑片（Slice），由于一个时间点只能扫描一张脑片，如果是按照顺序一张一张扫下去的话，每张脑片之间的扫面时间点都会有区别。比如最顶端的脑片的扫描时间会相对低端的脑片延迟2s，需要做时间回归。通过插值等方法来获得三个脑片相同时间点的数据，通过与未知点相邻的已知点的信号值来预测未知点的值，通常有线性函数和三角函数。</p>
<h3 id="头动校正"><a href="#头动校正" class="headerlink" title="头动校正"></a>头动校正</h3><p>​        通常使用刚性变换把所有的图像中的脑都固定在同一个靶位置，一个刚体变换包含6个自由度，即关于X、Y、Z轴三个方向的平移以及围绕这三个轴的旋转。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/17/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84BWDSP%E6%8C%87%E4%BB%A4%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95%E4%BC%98%E5%8C%96%E7%A0%94%E7%A9%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/17/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84BWDSP%E6%8C%87%E4%BB%A4%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95%E4%BC%98%E5%8C%96%E7%A0%94%E7%A9%B6/" class="post-title-link" itemprop="url">基于图神经网络的BWDSP指令选择方法优化研究-论文阅读2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-01-17 08:55:52 / 修改时间：08:56:28" itemprop="dateCreated datePublished" datetime="2023-01-17T08:55:52+08:00">2023-01-17</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/14/Pytorch2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/14/Pytorch2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" class="post-title-link" itemprop="url">Pytorch2-梯度下降</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-01-14 21:10:48" itemprop="dateCreated datePublished" datetime="2023-01-14T21:10:48+08:00">2023-01-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-15 09:02:42" itemprop="dateModified" datetime="2023-01-15T09:02:42+08:00">2023-01-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h2><p>​        梯度为导数变化最大的值，其方向为导数变化最大的方向。</p>
<script type="math/tex; mode=display">
$\frac{\partial f}{\partial x}=\lim_{\triangle x\rightarrow\infty}\frac{f(x+\triangle x)-f(x)}{\triangle x}$</script><script type="math/tex; mode=display">
</script><p>当△x&gt;0时，对于增函数，梯度为上升方向，对于减函数，梯度为下降方向。因此需要取梯度下降的方向即梯度的反方向作为变化方向。所取梯度即为∂cost∂ω\frac{\partial cost}{\partial \omega}∂ω∂cost，更新公式为</p>
<script type="math/tex; mode=display">
\omega = \omega - \alpha \frac{\partial cost}{\partial \omega}</script><p>其中α\alphaα为学习率即所下降的步长==不能取太大==，否则会很难收敛。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/image-20230114213115420.png" alt="image-20230114213115420"></p>
<h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><ul>
<li><p>梯度下降算法容易进入局部最优解</p>
<p>但在实际问题中的局部最优点较少，或者已经基本可以当成全局最优</p>
</li>
<li><p>梯度下降算法容易陷入鞍点，即梯度为0导致参数无法更新</p>
</li>
</ul>
<h3 id="梯度公式"><a href="#梯度公式" class="headerlink" title="梯度公式"></a>梯度公式</h3><script type="math/tex; mode=display">
$cost = \frac{1}{N} \displaystyle\sum_{n=1}^{N}(\widehat y_n-y_n)^2$</script><p>可知</p>
<script type="math/tex; mode=display">
$\frac{\partial cost(w)}{\partial \omega} = \frac{\partial}{\partial w} \frac{1}{N} \displaystyle\sum_{n=1}^{N}(\widehat y_n-y_n)^2$</script><p>其中</p>
<script type="math/tex; mode=display">
$\widehat y = \omega x$</script><p>则</p>
<script type="math/tex; mode=display">
$\frac{\partial cost(w)}{\partial \omega} = \frac{\partial}{\partial w} \frac{1}{N} \displaystyle\sum_{n=1}^{N}(\omega x_n-y_n)^2$</script><script type="math/tex; mode=display">
$\frac{\partial cost(w)}{\partial \omega}
=\frac{1}{N} \displaystyle\sum_{n=1}^{N}\frac{\partial}{\partial w}(\omega x_n-y_n)^2$</script><script type="math/tex; mode=display">
$\frac{\partial cost(w)}{\partial \omega} 
=\frac{1}{N} \displaystyle\sum_{n=1}^{N}2 x_n(x_n \omega - y_n)$</script><p>即</p>
<script type="math/tex; mode=display">
$\omega = \omega - \alpha \frac{1}{N} \displaystyle\sum_{n=1}^{N}2 x_n(x_n \omega - y_n)$</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line">_cost = []</span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line"><span class="comment">#前馈计算</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"><span class="comment">#求MSE</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cost</span>(<span class="params">xs, ys</span>):</span><br><span class="line">    cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs,ys):</span><br><span class="line">        y_pred = forward(x)</span><br><span class="line">        cost += (y_pred-y) ** <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> cost/<span class="built_in">len</span>(xs)</span><br><span class="line"><span class="comment">#求梯度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">xs, ys</span>):</span><br><span class="line">    grad = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs,ys):</span><br><span class="line">        temp = forward(x)</span><br><span class="line">        grad += <span class="number">2</span>*x*(temp-y)</span><br><span class="line">    <span class="keyword">return</span> grad / <span class="built_in">len</span>(xs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">     cost_val = cost(x_data, y_data)</span><br><span class="line">     _cost.append(cost_val)</span><br><span class="line">     grad_val = gradient(x_data, y_data)</span><br><span class="line">     w -= <span class="number">0.01</span>*grad_val</span><br><span class="line">     <span class="built_in">print</span>(<span class="string">"Epoch: "</span>,epoch, <span class="string">"w = "</span>,w ,<span class="string">"loss = "</span>, cost_val)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Predict(after training)"</span>,<span class="number">4</span>,forward(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘图</span></span><br><span class="line">plt.plot(_cost,<span class="built_in">range</span>(<span class="number">100</span>))</span><br><span class="line">plt.ylabel(<span class="string">"Cost"</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epoch'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/image-20230114213749302.png" alt="image-20230114213749302"></p>
<h3 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><p>​     随机选单个样本的损失为标准。即原公式变为：</p>
<script type="math/tex; mode=display">
$\omega = \omega - \alpha \frac{\partial loss}{\partial \omega}$</script><p>其中：</p>
<script type="math/tex; mode=display">
$\frac{\partial loss_n}{\partial \omega} = 2 x_n(x_n \omega - y_n)$</script><p>​        随机梯度下降又可能跨越鞍点，神经网络经常使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机梯度下降</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line">_cost = []</span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line"><span class="comment">#前馈计算</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"><span class="comment">#求单个loss</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred-y) ** <span class="number">2</span></span><br><span class="line"><span class="comment">#求梯度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*x*(x*w-y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Predict(after training)"</span>,<span class="number">4</span>,forward(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        grad=gradient(x,y) </span><br><span class="line">        w -= <span class="number">0.01</span>*grad</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"\tgrad:  "</span>,x,y,grad)</span><br><span class="line">        l = loss(x,y)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"progress: "</span>,epoch,<span class="string">"w="</span>,w,<span class="string">"loss="</span>,l)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Predict(after training)"</span>,<span class="number">4</span>,forward(<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<h3 id="批量梯度下降（mini-batch）"><a href="#批量梯度下降（mini-batch）" class="headerlink" title="批量梯度下降（mini-batch）"></a>批量梯度下降（mini-batch）</h3><p>​        普通的梯度下降算法利用数据整体，不容易避免鞍点，算法性能上欠佳，但算法效率高，可以并行运算。随机梯度下降需要利用每个的单个数据，虽然算法性能上良好，但计算过程往往相扣无法将样本抽离开，因此算法效率低，时间复杂度高。</p>
<p>​        综上可采取折中方法，即==批量梯度下降算法==。将若干个样本分为一组，纪录一组的梯度用以代替随机梯度下降中的单个样本。该方法常用，也是默认接口。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/14/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/14/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81/" class="post-title-link" itemprop="url">Pytorch1-线性模型代码</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-01-14 20:27:08 / 修改时间：21:04:11" itemprop="dateCreated datePublished" datetime="2023-01-14T20:27:08+08:00">2023-01-14</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"><span class="comment">#前馈计算</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"><span class="comment">#求loss</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred-y)*(y_pred-y)</span><br><span class="line"></span><br><span class="line">w_list = []</span><br><span class="line">mse_list = []</span><br><span class="line"><span class="comment">#从0.0一直到4.1以0.1为间隔进行w的取样</span></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0.0</span>,<span class="number">4.1</span>,<span class="number">0.1</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;w=&quot;</span>, w)</span><br><span class="line">    l_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_val,y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        y_pred_val = forward(x_val)</span><br><span class="line">        loss_val = loss(x_val,y_val)</span><br><span class="line">        l_sum += loss_val</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span>,x_val,y_val,y_pred_val,loss_val)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;MSE=&quot;</span>,l_sum/<span class="number">3</span>)</span><br><span class="line">    w_list.append(w)</span><br><span class="line">    mse_list.append(l_sum/<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘图</span></span><br><span class="line">plt.plot(w_list,mse_list)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Loss&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/14/%E9%9D%A2%E5%90%91%E8%87%AA%E9%97%AD%E7%97%87%E8%BE%85%E5%8A%A9%E8%AF%8A%E6%96%AD%E7%9A%84%E8%81%94%E5%90%88%E7%BB%84%E7%A8%80%E7%96%8FTSK%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yi-yi1">
      <meta itemprop="description" content="种一棵树最好的时间是现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不就熬个夜">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/01/14/%E9%9D%A2%E5%90%91%E8%87%AA%E9%97%AD%E7%97%87%E8%BE%85%E5%8A%A9%E8%AF%8A%E6%96%AD%E7%9A%84%E8%81%94%E5%90%88%E7%BB%84%E7%A8%80%E7%96%8FTSK%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB1/" class="post-title-link" itemprop="url">面向自闭症辅助诊断的联合组稀疏TSK建模方法(论文阅读1)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-01-14 11:07:58" itemprop="dateCreated datePublished" datetime="2023-01-14T11:07:58+08:00">2023-01-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-15 09:02:29" itemprop="dateModified" datetime="2023-01-15T09:02:29+08:00">2023-01-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>​    静息态功能磁共振成像（resting-statefunctional magnetic resonance imaging，rs-fMRI）技术能够在无创伤、无辐射条件下，通过检测血氧水平获得高分辨率图像来体现大脑活动的异常。自闭症的形成与大脑的形态结构变化密切相关，患者脑功能连接方面存在着不同脑区之间近距离连接过度、远距离连接不足等问题。</p>
<p>​    TSK 模糊系统是一种高效的模糊推理系统，对解决不确定性问题具有很好的针对性。其核心思想是通过对训练数据的输入/输出集合进行划分来提取“if-then”模糊规则，在此基础上进行模糊规则后件参数的学习来挖掘输入数据和输出数据之间的映射关系。联合组稀疏TSK模糊系统建模方法，基于TSK模糊系统理论框架，结合特征之间的关联信息学习新的权重系数，使用一种全新的方式来构造不同模糊规则后件参数之间的联合组稀疏正则化项，引导规则内特征和规则间特征的联合选择，从而降低不确定性。</p>
<h2 id="2-数据处理"><a href="#2-数据处理" class="headerlink" title="2 数据处理"></a>2 数据处理</h2><h3 id="2-1-数据预处理"><a href="#2-1-数据预处理" class="headerlink" title="2.1 数据预处理"></a>2.1 数据预处理</h3><h3 id="2-2-面向rs-fMRI数据的特征提取"><a href="#2-2-面向rs-fMRI数据的特征提取" class="headerlink" title="2.2 面向rs-fMRI数据的特征提取"></a>2.2 面向rs-fMRI数据的特征提取</h3><p>​    计算每个样本个脑区之间的Pearson相关系数，得到功能连接矩阵，该矩阵表示脑区之间的线性相关程度，具有对称性。</p>
<p>​    在训练集中选出相关系数最大的P个特征组成新的训练集，根据索引更新对应的验验证集的测试集，</p>
<h2 id="3-特征关联诱导联合组稀疏TSK模糊系统"><a href="#3-特征关联诱导联合组稀疏TSK模糊系统" class="headerlink" title="3 特征关联诱导联合组稀疏TSK模糊系统"></a>3 特征关联诱导联合组稀疏TSK模糊系统</h2><h3 id="3-1-TSK模糊系统"><a href="#3-1-TSK模糊系统" class="headerlink" title="3.1 TSK模糊系统"></a>3.1 TSK模糊系统</h3><h3 id="3-2-规则间联合组稀疏学习"><a href="#3-2-规则间联合组稀疏学习" class="headerlink" title="3.2 规则间联合组稀疏学习"></a>3.2 规则间联合组稀疏学习</h3><h3 id="3-3-权重系数学习"><a href="#3-3-权重系数学习" class="headerlink" title="3.3 权重系数学习"></a>3.3 权重系数学习</h3><h3 id="3-4-目标函数优化"><a href="#3-4-目标函数优化" class="headerlink" title="3.4 目标函数优化"></a>3.4 目标函数优化</h3><h2 id="4-实验设置"><a href="#4-实验设置" class="headerlink" title="4 实验设置"></a>4 实验设置</h2><p>​    常用指标为敏感度SEN(sensitivity)和特异性SPE(apecificity)，SEN越高，漏诊率越低，确诊病人的可能性越大；SPE越高，误诊率越低。分辨正常人的能力越低。ROC曲线的面积越大，AUC值越大，，所对应的算法的诊断性能越好。</p>
<p>​    在训练集上训练模型，在验证集上进行网格参数寻优，在测试集上评估分类性能。采用阈值为0.5的sigmoid函数实现分类，即大于等于0.5时为正例，小于0.5时为负例。</p>
<p>​    实验表明，联合组稀疏非线性模糊分类方法JGSL-TSK能够有效改进分类模型在辅助诊断自闭症上的性能。模糊推理系统针对解决不确定性问题具有更好的非线性逼近能力。</p>
<p><img src="https://raw.githubusercontent.com/yi-yi1/blogImage/main/image-20230114112041004.png" alt=""></p>
<p>​    JGSL-TSK在进行特征提取的基础上，不仅通过稀疏化精度矩阵提取规则内特征之间的相关信息，而且在构造分类模型的过程中，引入L1、L2正则化实现规则间的特征选择，从而有效地降低噪声影响，更好地利用特征间的关联信息提高确诊功能，且收敛速度快，在迭代5~10次左右目标函数便趋于稳定，实用性较好。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">yi-yi1</p>
  <div class="site-description" itemprop="description">种一棵树最好的时间是现在</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yi-yi1</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
